# Streaming-Data-Pipeline-Data-Engineering-Project
# Introduction
This project involves building a comprehensive end-to-end data engineering pipeline. It covers data ingestion, processing, and storage using a robust tech stack and Docker for scalability.
# System Architecture 
![image](https://github.com/user-attachments/assets/c32baf7b-5a31-4d95-b279-b6004d1d3841)
# Technologies
Data Source: Randomuser.me API for generating user data.
Apache Airflow: Orchestrates the pipeline and stores data in PostgreSQL.
Apache Kafka & Zookeeper: Manages real-time data streaming from PostgreSQL.
Control Center & Schema Registry: Monitors and manages Kafka schemas.
Apache Spark: Processes data with master and worker nodes.
Cassandra: Stores the processed data.

